{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 3: Dyadic Regression\n",
    "#### Econometric Methods for Social Spillovers and Networks\n",
    "#### University of St. Gallen, September 28th to October 6th, 2020\n",
    "##### _Bryan S. Graham, UC - Berkeley, bgraham@econ.berkeley.edu_\n",
    "<br>\n",
    "<br>\n",
    "This is the third of a series of iPython Jupyter notebooks designed to accompany a series of instructional lectures given (virtually!) at the St. Gallen University from September 28th and October 6th, 2020. The scripts below were written for Python 3.6. The Anaconda distribution of Python, available at https://www.continuum.io/downloads, comes bundled with most the scientific computing packages used in these notebooks.\n",
    "<br>\n",
    "<br>\n",
    "For more information about the course please visit my webpage at http://bryangraham.github.io/econometrics/."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code citation:\n",
    "Graham, Bryan S. (2020). \"Notebook 2: Dyadic Regression: St. Gallen University Econometric Methods for Social Spillovers and Networks Course Jupyter Notebook,\" (Version 1.0) [Computer program]. Available at http://bryangraham.github.io/econometrics/ (Accessed 20 September 2020)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dyadic regression and the gravity model of international trade\n",
    "\n",
    "Jo√£o Santos Silva and Silvana Tenreyro (2006, _Review of Economics and Statistics_) advocate the application of Poisson regression methods to the analysis of trade flows across countries. Their work has been influential in the empirical trade literature. Google Scholar calls their work a \"classic paper\" in economics (see this [blog post](https://scholar.googleblog.com/2017/06/classic-papers-articles-that-have-stood.html) for more information on Google Scholar \"classic papers\"). An open access copy of the paper can be found [here](https://www.mitpressjournals.org/doi/pdf/10.1162/rest.88.4.641). Replication data and additional information about the dataset used in the paper can be found [here](http://personal.lse.ac.uk/tenreyro/lgw.html).    \n",
    "\n",
    "In this notebook we will use the Santos Silva and Tenreyro (2006) dataset to illustrate our results on estimation and inference of dyadic regression models. The dataset is located in the \"data\" directory of the Github repository. You'll need to download the data and adjust the **data** directory defined below to run this notebook on your computer.\n",
    "\n",
    "**References**    \n",
    "\n",
    "Santos Silva, J.M.C. and Tenreyro, Silvana. (2006). \"The Log of Gravity\", _Review of Economics and Statistics_ 88 (4): 641 - 658."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Direct Python to plot all figures inline (i.e., not in a separate window)\n",
    "%matplotlib inline\n",
    "\n",
    "# Main scientific computing modules\n",
    "# Load library dependencies\n",
    "import time\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import pandas as pd\n",
    "import itertools as it\n",
    "\n",
    "# Import matplotlib \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# networkx module for the analysis of network data\n",
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the **dyadic_regression()** function in the _netrics_ package. This package has dependencies with the _ipt_ package. We therefore add the directories with these two codebases to our system path and then load them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Append location of ipt module base directory to system path\n",
    "# NOTE: only required if permanent install not made (see comments above)\n",
    "import sys\n",
    "sys.path.append('/Users/bgraham/Dropbox/Sites/software/ipt/')\n",
    "sys.path.append('/Users/bgraham/Dropbox/Sites/software/netrics/')\n",
    "\n",
    "# Load ipt and netrics modules\n",
    "import ipt as ipt\n",
    "import netrics as netrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Location of Santos Silva and Tenreyro (2006) replication data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory where graphics files will be saved (and where example data are stored)\n",
    "data =     '/Users/bgraham/Dropbox/Teaching/Short_Courses/St_Gallen/2018/Data/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load dataset into a pandas dataframe. Set (dyadic) multi-index and sort. Print out first few lines of the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>s1_im</th>\n",
       "      <th>s2_ex</th>\n",
       "      <th>border</th>\n",
       "      <th>ex_feenstra</th>\n",
       "      <th>im_feenstra</th>\n",
       "      <th>landl_im</th>\n",
       "      <th>landl_ex</th>\n",
       "      <th>trade</th>\n",
       "      <th>lyim</th>\n",
       "      <th>lyex</th>\n",
       "      <th>...</th>\n",
       "      <th>imuis</th>\n",
       "      <th>exuis</th>\n",
       "      <th>imcacm</th>\n",
       "      <th>excacm</th>\n",
       "      <th>imcaricom</th>\n",
       "      <th>excaricom</th>\n",
       "      <th>imopen</th>\n",
       "      <th>exopen</th>\n",
       "      <th>open_wto</th>\n",
       "      <th>comfrt_wto</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>s1_im</th>\n",
       "      <th>s2_ex</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">2</th>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>130120</td>\n",
       "      <td>580080</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>237</td>\n",
       "      <td>6.735698</td>\n",
       "      <td>7.407237</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>160240</td>\n",
       "      <td>580080</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6.735698</td>\n",
       "      <td>6.500353</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>330320</td>\n",
       "      <td>580080</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>6.735698</td>\n",
       "      <td>8.661679</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>710360</td>\n",
       "      <td>580080</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6.735698</td>\n",
       "      <td>9.833121</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>550400</td>\n",
       "      <td>580080</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>12355</td>\n",
       "      <td>6.735698</td>\n",
       "      <td>10.222010</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows √ó 63 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             s1_im  s2_ex  border  ex_feenstra  im_feenstra  landl_im  \\\n",
       "s1_im s2_ex                                                             \n",
       "2     3          2      3       0       130120       580080         0   \n",
       "      6          2      6       0       160240       580080         0   \n",
       "      8          2      8       0       330320       580080         0   \n",
       "      11         2     11       0       710360       580080         0   \n",
       "      12         2     12       0       550400       580080         0   \n",
       "\n",
       "             landl_ex  trade      lyim       lyex  ...  imuis  exuis  imcacm  \\\n",
       "s1_im s2_ex                                        ...                         \n",
       "2     3             0    237  6.735698   7.407237  ...      0      0       0   \n",
       "      6             0      0  6.735698   6.500353  ...      0      0       0   \n",
       "      8             0     20  6.735698   8.661679  ...      0      0       0   \n",
       "      11            0      0  6.735698   9.833121  ...      0      0       0   \n",
       "      12            1  12355  6.735698  10.222010  ...      0      0       0   \n",
       "\n",
       "             excacm  imcaricom  excaricom  imopen  exopen  open_wto  \\\n",
       "s1_im s2_ex                                                           \n",
       "2     3           0          0          0       0       0         1   \n",
       "      6           0          0          0       0       0         0   \n",
       "      8           0          0          0       0       0         0   \n",
       "      11          0          0          0       0       1         1   \n",
       "      12          0          0          0       0       1         1   \n",
       "\n",
       "             comfrt_wto  \n",
       "s1_im s2_ex              \n",
       "2     3               0  \n",
       "      6               0  \n",
       "      8               0  \n",
       "      11              0  \n",
       "      12              0  \n",
       "\n",
       "[5 rows x 63 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LogOfGravity = pd.read_stata(data+\"Log of Gravity.dta\")\n",
    "LogOfGravity.set_index(['s1_im', 's2_ex'], drop = False, inplace = True)          # Set dataframe multi-index\n",
    "LogOfGravity.sort_index(level = ['s1_im', 's2_ex'], inplace = True)\n",
    "LogOfGravity.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **trade** variable (*Y*) measures trade from exporter *i* to importer *j* in thousands of US dollars. Dividing by 1,000 scales the variable better for estimation. The set of covariates included in *W* correspond to those in used in Table 3 of Santos Silva and Tenreyro (2006, p. 650). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = LogOfGravity['trade'].copy(deep=True)/1000\n",
    "#W = LogOfGravity[['lyex', 'lyim', 'ldist']].copy(deep=True)\n",
    "W = LogOfGravity[['lypex', 'lypim', 'lyex', 'lyim', 'ldist', 'border', 'comlang', 'colony', \\\n",
    "                  'landl_ex', 'landl_im', 'lremot_ex', 'lremot_im', 'comfrt_wto', 'open_wto']].copy(deep=True)\n",
    "W['constant'] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **dyadic_regression()** function fits simple linear, logistic and poisson regression models to dyadic data by the method of (dyadic or pairwise) composite maximum likelihood estimation. The function can accommodate directed outcomes, in which case a total of N(N-1) directed outcomes are observed (with N the number of agents), as well as undirected outcomes, in which case 0.5N(N-1) outcomes are observed. The program does not, as of yet, accommodate the presence of missing outcomes.    \n",
    "\n",
    "The function allows for three methods of standard error construction. All three of these methods are described in the forthcoming _Handbook of Econometrics_ chapter on networks by Graham (forthcoming). The first method assumes independence (conditional on covariates) across dyads. In the case of directed outcome data this does allow for dependence of $ Y_{ij} $ with $ Y_{ji} $ but rules out dependence between, say, $ Y_{ij} $ with $ Y_{ik} $. The other two estimators allow for dependence across any two dyads sharing at least one agent in common. The first of these corresponds to the Jackknife estimate of the leading term in the asymptotic variance of $ \\sqrt{N}\\left(\\hat{\\theta}-\\theta_{0}\\right) $. The second estimate is a bias corrected variance estimate which also includes (estimates of) higher order variance terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function dyadic_regression in module netrics.dyadic_regression:\n",
      "\n",
      "dyadic_regression(Y, R, regmodel='normal', directed=True, nocons=False, silent=False, cov='DR_bc')\n",
      "    AUTHOR: Bryan S. Graham, UC - Berkeley, bgraham@econ.berkeley.edu, July 2017\n",
      "            (revised September 2018)\n",
      "    PYTHON 3.6\n",
      "    \n",
      "    This function computes dyadic regression estimates under linear, logit or poisson conditional mean\n",
      "    models. The outcome may be directed or undirected. The outcome model is E[Y_ij|R_ij=r] = r'theta, \n",
      "    exp(r'theta)/[1+exp(r'theta)], or exp(r'theta) depending on whether the regression model is chosen to be\n",
      "    linear, logit or poisson. A variety of standard error estimates, as described further below, are\n",
      "    reported along with point estimates of theta. Function only works with balanced datasets at the present \n",
      "    time. The basic procedure is as described in the _Handbook of Econometrics_ chaper by\n",
      "    Graham (forthcoming),\n",
      "        \n",
      "    N = number of agents\n",
      "    n = N(N-1), the number of *directed* dyads, or n = 0.5N(N-1), the number of *undirected* dyads\n",
      "    \n",
      "    \n",
      "    INPUTS:\n",
      "    -------\n",
      "    Y              :  n-length Pandas series with outcome for each\n",
      "                      (directed) dyad as elements  \n",
      "    R              :  n x K Pandas dataframe / regressor matrix\n",
      "    regmodel       :  Model for E[Y_ij|R_ij]: 'normal', 'logit' or 'poisson'\n",
      "    directed       :  if True then assume N*(N-1) directed outcomes present, \n",
      "                      otherwise assume 0.5*N(N-1) undirected outcomes are present\n",
      "    nocons         :  if True do NOT append a constant to the regressor\n",
      "                      matrix (default is to append a constant)\n",
      "    silent         :  if True suppress optimization and estimation output\n",
      "    cov            :  covariance matrix estimator\n",
      "                      'ind', 'DR', 'DR_bc' are allowable choices (see below)\n",
      "    \n",
      "    \n",
      "    The three variance-covariance matrices are as described in Graham (forthcoming).\n",
      "    'ind' assumes independence across dyads (note this corresponds to \"clustering\"\n",
      "    of dyads in the directed case where each dyad is observed twice); `DR' allows \n",
      "    for dependence across dyads sharing an agent in common. It corresponds to the \n",
      "    usual Jackknife variance estimate of the leading term in the asymptotic variance \n",
      "    expression. 'DR_bc' is a bias-corrected variance estimate.                     \n",
      "    \n",
      "    \n",
      "    OUTPUTS:\n",
      "    --------\n",
      "    theta_DR        : K x 1 vector of coefficient estimates \n",
      "    vcov_theta_DR   : K x K variance-covariance matrix \n",
      "       \n",
      "    FUNCTIONS CALLED : ...ols(), logit(), poisson()... \n",
      "    ----------------   ...print_coef()...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(netrics.dyadic_regression)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We begin by attempting to replicate column 6 of Table 3 in Santos Silva and Tenreyro (2006, p. 650). The point estimates reported below agree with the ones reported in their table up to the three decimal places reported (with the exception of a few cases). Note the summary statistics computed using the downloaded replication file differ very slightly from those reported in Table A5 of the paper. This, along with optimization errors, likely account for the small discrepancies.    \n",
    "\n",
    "The standard errors reported below differ from those in the paper. This is because the paper uses a basic heteroscedastic robust variance-covariance estimate (see p. 645). This variance estimator requires independence of $ Y_{ij} $ and $ Y_{ji} $. In subsequent work Santos Silva and Tenreyro (2010, _Annual Review of Economics_) instead \"cluster\" on dyads; this corresponds to choosing the 'ind' option for variance estimation in the **dyadic_regression()** command. Of course neither of these two variance estimators make particularly plausible assumptions about the dependence structure of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fisher-Scoring Derivative check (2-norm): 0.10803891\n",
      "Value of -logL = -15183641.135391,  2-norm of score = 44724805.947635\n",
      "Value of -logL = -15419066.810454,  2-norm of score = 18265431.975691\n",
      "Value of -logL = -17556821.955322,  2-norm of score = 23277098.693045\n",
      "Value of -logL = -17607826.140885,  2-norm of score = 7930537.958928\n",
      "Value of -logL = -19468215.892431,  2-norm of score = 12731747.589155\n",
      "Value of -logL = -20885089.422488,  2-norm of score = 96885103.159100\n",
      "Value of -logL = -21421054.591814,  2-norm of score = 17605387.771348\n",
      "Value of -logL = -21445918.161899,  2-norm of score = 1726319.684780\n",
      "Value of -logL = -22293590.325171,  2-norm of score = 9227486.656333\n",
      "Value of -logL = -22300652.101634,  2-norm of score = 977704.460716\n",
      "Value of -logL = -22562892.771831,  2-norm of score = 1862539.510670\n",
      "Value of -logL = -22563241.013342,  2-norm of score = 794371.372568\n",
      "Value of -logL = -22766083.919457,  2-norm of score = 1448761.458606\n",
      "Value of -logL = -22766287.378921,  2-norm of score = 552957.528116\n",
      "Value of -logL = -22883738.061587,  2-norm of score = 904006.462449\n",
      "Value of -logL = -22883819.957777,  2-norm of score = 389736.753520\n",
      "Value of -logL = -22933680.366331,  2-norm of score = 321195.279201\n",
      "Value of -logL = -23004232.409019,  2-norm of score = 1346230.764932\n",
      "Value of -logL = -23004385.658336,  2-norm of score = 132301.493720\n",
      "Value of -logL = -23010544.147655,  2-norm of score = 118628.152474\n",
      "Value of -logL = -23021494.082717,  2-norm of score = 114069.666755\n",
      "Value of -logL = -23036980.152110,  2-norm of score = 311299.649694\n",
      "Value of -logL = -23036988.480312,  2-norm of score = 46041.474887\n",
      "Value of -logL = -23351946.960505,  2-norm of score = 2250821.962420\n",
      "Value of -logL = -23352368.378869,  2-norm of score = 60454.637969\n",
      "Value of -logL = -23420137.709968,  2-norm of score = 218165.471329\n",
      "Value of -logL = -23420141.967660,  2-norm of score = 56103.919880\n",
      "Value of -logL = -23420292.723948,  2-norm of score = 38592.568182\n",
      "Value of -logL = -23511187.197106,  2-norm of score = 551983.219116\n",
      "Value of -logL = -23511212.795362,  2-norm of score = 37184.721830\n",
      "Value of -logL = -23570506.860936,  2-norm of score = 485874.750492\n",
      "Value of -logL = -23570526.680453,  2-norm of score = 33739.831506\n",
      "Value of -logL = -23583233.333985,  2-norm of score = 49351.558985\n",
      "Value of -logL = -23583315.947482,  2-norm of score = 16204.162263\n",
      "Value of -logL = -23596115.390269,  2-norm of score = 63713.253574\n",
      "Value of -logL = -23596115.743264,  2-norm of score = 12683.248112\n",
      "Value of -logL = -23609066.900354,  2-norm of score = 111816.638685\n",
      "Value of -logL = -23609067.950886,  2-norm of score = 8271.281911\n",
      "Value of -logL = -23611754.463447,  2-norm of score = 11100.477052\n",
      "Value of -logL = -23616165.892203,  2-norm of score = 49342.196575\n",
      "Value of -logL = -23616166.097306,  2-norm of score = 4530.091379\n",
      "Value of -logL = -23616703.248047,  2-norm of score = 4153.042797\n",
      "Value of -logL = -23617658.939966,  2-norm of score = 6964.796675\n",
      "Value of -logL = -23617668.197372,  2-norm of score = 3110.453214\n",
      "Value of -logL = -23618031.329727,  2-norm of score = 3071.693210\n",
      "Value of -logL = -23618621.881177,  2-norm of score = 7328.692450\n",
      "Value of -logL = -23618621.885897,  2-norm of score = 1674.853374\n",
      "Value of -logL = -23618799.179275,  2-norm of score = 1568.873689\n",
      "Value of -logL = -23618971.440603,  2-norm of score = 7453.047501\n",
      "Value of -logL = -23618971.445242,  2-norm of score = 121.245493\n",
      "Value of -logL = -23618971.446429,  2-norm of score = 35.967185\n",
      "Value of -logL = -23618971.447437,  2-norm of score = 13.996046\n",
      "Value of -logL = -23618971.447592,  2-norm of score = 5.970053\n",
      "Value of -logL = -23618971.447632,  2-norm of score = 2.446420\n",
      "Value of -logL = -23618971.447974,  2-norm of score = 1.730074\n",
      "Value of -logL = -23618971.448313,  2-norm of score = 0.789717\n",
      "Value of -logL = -23618971.448313,  2-norm of score = 0.180677\n",
      "Value of -logL = -23618971.448314,  2-norm of score = 0.029735\n",
      "Value of -logL = -23618971.448314,  2-norm of score = 0.004030\n",
      "Value of -logL = -23618971.448314,  2-norm of score = 0.000002\n",
      "Value of -logL = -23618971.448314,  2-norm of score = 0.000001\n",
      "Optimization terminated successfully.\n",
      "         Current function value: -23618971.448314\n",
      "         Iterations: 61\n",
      "         Function evaluations: 145\n",
      "         Gradient evaluations: 145\n",
      "         Hessian evaluations: 61\n",
      "\n",
      "-------------------------------------------------------------------------------------------\n",
      "- DYADIC REGRESSION ESTIMATION RESULTS                                                    -\n",
      "- (poisson regression model)                                                     -\n",
      "-------------------------------------------------------------------------------------------\n",
      "\n",
      "Number of agents,           N :             136\n",
      "Number of dyads,            n :          18,360\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------------------\n",
      "\n",
      "Independent variable       Coef.    ( Std. Err.)     (0.95 Confid. Interval )\n",
      "-------------------------------------------------------------------------------------------\n",
      "lypex                      0.732481 (  0.020379)     (  0.692539 ,  0.772423)\n",
      "lypim                      0.741078 (  0.021061)     (  0.699798 ,  0.782358)\n",
      "lyex                       0.156712 (  0.041265)     (  0.075833 ,  0.237591)\n",
      "lyim                       0.135018 (  0.030035)     (  0.076150 ,  0.193887)\n",
      "ldist                     -0.783801 (  0.073203)     ( -0.927276 , -0.640325)\n",
      "border                     0.192911 (  0.137228)     ( -0.076051 ,  0.461873)\n",
      "comlang                    0.745984 (  0.185844)     (  0.381736 ,  1.110232)\n",
      "colony                     0.025006 (  0.202066)     ( -0.371035 ,  0.421048)\n",
      "landl_ex                  -0.863474 (  0.160037)     ( -1.177140 , -0.549808)\n",
      "landl_im                  -0.696420 (  0.145674)     ( -0.981937 , -0.410904)\n",
      "lremot_ex                  0.659840 (  0.139138)     (  0.387135 ,  0.932545)\n",
      "lremot_im                  0.561500 (  0.124589)     (  0.317309 ,  0.805691)\n",
      "comfrt_wto                 0.181107 (  0.115616)     ( -0.045497 ,  0.407711)\n",
      "open_wto                  -0.106819 (  0.166685)     ( -0.433516 ,  0.219878)\n",
      "constant                 -39.233858 (  2.395350)     (-43.928659 ,-34.539058)\n",
      "\n",
      "-------------------------------------------------------------------------------------------\n",
      "NOTE: Standard errors assume independence across dyads.\n"
     ]
    }
   ],
   "source": [
    "[theta_DR, vcov_theta_DR]= netrics.dyadic_regression(Y, W, regmodel='poisson', directed=True, nocons=True, \\\n",
    "                                                     silent=False, cov='ind')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we repeat estimation, but this time compute standard errors using the bias corrected variance estimate mentioned above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fisher-Scoring Derivative check (2-norm): 0.10803891\n",
      "Value of -logL = -15183641.135391,  2-norm of score = 44724805.947635\n",
      "Value of -logL = -15419066.810454,  2-norm of score = 18265431.975691\n",
      "Value of -logL = -17556821.955322,  2-norm of score = 23277098.693045\n",
      "Value of -logL = -17607826.140885,  2-norm of score = 7930537.958928\n",
      "Value of -logL = -19468215.892431,  2-norm of score = 12731747.589155\n",
      "Value of -logL = -20885089.422488,  2-norm of score = 96885103.159100\n",
      "Value of -logL = -21421054.591814,  2-norm of score = 17605387.771348\n",
      "Value of -logL = -21445918.161899,  2-norm of score = 1726319.684780\n",
      "Value of -logL = -22293590.325171,  2-norm of score = 9227486.656333\n",
      "Value of -logL = -22300652.101634,  2-norm of score = 977704.460716\n",
      "Value of -logL = -22562892.771831,  2-norm of score = 1862539.510670\n",
      "Value of -logL = -22563241.013342,  2-norm of score = 794371.372568\n",
      "Value of -logL = -22766083.919457,  2-norm of score = 1448761.458606\n",
      "Value of -logL = -22766287.378921,  2-norm of score = 552957.528116\n",
      "Value of -logL = -22883738.061587,  2-norm of score = 904006.462449\n",
      "Value of -logL = -22883819.957777,  2-norm of score = 389736.753520\n",
      "Value of -logL = -22933680.366331,  2-norm of score = 321195.279201\n",
      "Value of -logL = -23004232.409019,  2-norm of score = 1346230.764932\n",
      "Value of -logL = -23004385.658336,  2-norm of score = 132301.493720\n",
      "Value of -logL = -23010544.147655,  2-norm of score = 118628.152474\n",
      "Value of -logL = -23021494.082717,  2-norm of score = 114069.666755\n",
      "Value of -logL = -23036980.152110,  2-norm of score = 311299.649694\n",
      "Value of -logL = -23036988.480312,  2-norm of score = 46041.474887\n",
      "Value of -logL = -23351946.960505,  2-norm of score = 2250821.962420\n",
      "Value of -logL = -23352368.378869,  2-norm of score = 60454.637969\n",
      "Value of -logL = -23420137.709968,  2-norm of score = 218165.471329\n",
      "Value of -logL = -23420141.967660,  2-norm of score = 56103.919880\n",
      "Value of -logL = -23420292.723948,  2-norm of score = 38592.568182\n",
      "Value of -logL = -23511187.197106,  2-norm of score = 551983.219116\n",
      "Value of -logL = -23511212.795362,  2-norm of score = 37184.721830\n",
      "Value of -logL = -23570506.860936,  2-norm of score = 485874.750492\n",
      "Value of -logL = -23570526.680453,  2-norm of score = 33739.831506\n",
      "Value of -logL = -23583233.333985,  2-norm of score = 49351.558985\n",
      "Value of -logL = -23583315.947482,  2-norm of score = 16204.162263\n",
      "Value of -logL = -23596115.390269,  2-norm of score = 63713.253574\n",
      "Value of -logL = -23596115.743264,  2-norm of score = 12683.248112\n",
      "Value of -logL = -23609066.900354,  2-norm of score = 111816.638685\n",
      "Value of -logL = -23609067.950886,  2-norm of score = 8271.281911\n",
      "Value of -logL = -23611754.463447,  2-norm of score = 11100.477052\n",
      "Value of -logL = -23616165.892203,  2-norm of score = 49342.196575\n",
      "Value of -logL = -23616166.097306,  2-norm of score = 4530.091379\n",
      "Value of -logL = -23616703.248047,  2-norm of score = 4153.042797\n",
      "Value of -logL = -23617658.939966,  2-norm of score = 6964.796675\n",
      "Value of -logL = -23617668.197372,  2-norm of score = 3110.453214\n",
      "Value of -logL = -23618031.329727,  2-norm of score = 3071.693210\n",
      "Value of -logL = -23618621.881177,  2-norm of score = 7328.692450\n",
      "Value of -logL = -23618621.885897,  2-norm of score = 1674.853374\n",
      "Value of -logL = -23618799.179275,  2-norm of score = 1568.873689\n",
      "Value of -logL = -23618971.440603,  2-norm of score = 7453.047501\n",
      "Value of -logL = -23618971.445242,  2-norm of score = 121.245493\n",
      "Value of -logL = -23618971.446429,  2-norm of score = 35.967185\n",
      "Value of -logL = -23618971.447437,  2-norm of score = 13.996046\n",
      "Value of -logL = -23618971.447592,  2-norm of score = 5.970053\n",
      "Value of -logL = -23618971.447632,  2-norm of score = 2.446420\n",
      "Value of -logL = -23618971.447974,  2-norm of score = 1.730074\n",
      "Value of -logL = -23618971.448313,  2-norm of score = 0.789717\n",
      "Value of -logL = -23618971.448313,  2-norm of score = 0.180677\n",
      "Value of -logL = -23618971.448314,  2-norm of score = 0.029735\n",
      "Value of -logL = -23618971.448314,  2-norm of score = 0.004030\n",
      "Value of -logL = -23618971.448314,  2-norm of score = 0.000002\n",
      "Value of -logL = -23618971.448314,  2-norm of score = 0.000001\n",
      "Optimization terminated successfully.\n",
      "         Current function value: -23618971.448314\n",
      "         Iterations: 61\n",
      "         Function evaluations: 145\n",
      "         Gradient evaluations: 145\n",
      "         Hessian evaluations: 61\n",
      "\n",
      "-------------------------------------------------------------------------------------------\n",
      "- DYADIC REGRESSION ESTIMATION RESULTS                                                    -\n",
      "- (poisson regression model)                                                     -\n",
      "-------------------------------------------------------------------------------------------\n",
      "\n",
      "Number of agents,           N :             136\n",
      "Number of dyads,            n :          18,360\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------------------\n",
      "\n",
      "Independent variable       Coef.    ( Std. Err.)     (0.95 Confid. Interval )\n",
      "-------------------------------------------------------------------------------------------\n",
      "lypex                      0.732481 (  0.033088)     (  0.667630 ,  0.797332)\n",
      "lypim                      0.741078 (  0.035484)     (  0.671531 ,  0.810625)\n",
      "lyex                       0.156712 (  0.052329)     (  0.054149 ,  0.259274)\n",
      "lyim                       0.135018 (  0.046381)     (  0.044113 ,  0.225924)\n",
      "ldist                     -0.783801 (  0.063582)     ( -0.908420 , -0.659182)\n",
      "border                     0.192911 (  0.165108)     ( -0.130695 ,  0.516516)\n",
      "comlang                    0.745984 (  0.275508)     (  0.205999 ,  1.285969)\n",
      "colony                     0.025006 (  0.274542)     ( -0.513087 ,  0.563100)\n",
      "landl_ex                  -0.863474 (  0.128897)     ( -1.116107 , -0.610840)\n",
      "landl_im                  -0.696420 (  0.165449)     ( -1.020695 , -0.372146)\n",
      "lremot_ex                  0.659840 (  0.175423)     (  0.316018 ,  1.003662)\n",
      "lremot_im                  0.561500 (  0.169760)     (  0.228777 ,  0.894223)\n",
      "comfrt_wto                 0.181107 (  0.158613)     ( -0.129769 ,  0.491983)\n",
      "open_wto                  -0.106819 (  0.215419)     ( -0.529033 ,  0.315395)\n",
      "constant                 -39.233858 (  3.049340)     (-45.210454 ,-33.257263)\n",
      "\n",
      "-------------------------------------------------------------------------------------------\n",
      "NOTE: Standard errors allow for dependence across dyads with agents in common.\n",
      "      (Bias-corrected variance estimate). \n"
     ]
    }
   ],
   "source": [
    "[theta_DR, vcov_theta_DR]= netrics.dyadic_regression(Y, W, regmodel='poisson', directed=True, nocons=True, \\\n",
    "                                                     silent=False, cov='DR_bc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For some covariates the increase in the standard error associated with allowing for dependence across dyads with agents in common is very small. But in some cases the standard errors increase quite a bit. Consider *comfrt_wto*, which is a dummy variable equal to 1 if the two trading partners are party to a (common) preferential trading agreement and zero otherwise. The standard error on this coefficient increases by almost 50 percent. Furthermore this is an important trade policy variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Monte Carlo\n",
    "\n",
    "Next we conduct a short Monte Carlo experiment to illustrate the properties of inference methods based on the different variance-covariance estimates available in **dyad_regression()**. We set $ N = 200 $ and generate outcome data for all $N(N-1)$ ordered pairs of agents according to the outcome model:\n",
    "\n",
    "$$ Y_{ij}=\\exp\\left(\\theta_{1}W_{ij}+\\theta_{2}X_{i}+\\theta_{2}X_{j}\\right)A_{i}A_{j}U_{ij} $$\n",
    "\n",
    "Here $A_{i}$, for $i=1,...,N$, is a sequence of iid log normal random variables, each with mean 1 and scale parameter $ \\sigma_{A} $; $U_{ij}$ for $i=1,...,n$ with $n=N(N-1)$ is also sequence of iid log normal random variables, each with mean 1 and scale parameter $ \\sigma $.    \n",
    "\n",
    "Each agent is uniformaly at random assigned a location on the unit square, $W_{ij}$ equals the distance between agents *i* and *j* on that square; $X_{i}$ is a standard uniform random variable.    \n",
    "\n",
    "We set $\\theta_{1}=-1$, $\\theta_{1}=-1/2$ and $\\theta_{3}=1/2$. We set $ \\sigma = 1$ and $ \\sigma_{A} = 1/4$. This generates moderate, but meaningful, dependence across any two dyads sharing at least one agent in common.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time required f/ MC rep  100 of 1000: 0.7888047695159912\n",
      "Time required f/ MC rep  200 of 1000: 0.6665360927581787\n",
      "Time required f/ MC rep  300 of 1000: 0.8817238807678223\n",
      "Time required f/ MC rep  400 of 1000: 1.6328840255737305\n",
      "Time required f/ MC rep  500 of 1000: 1.4992878437042236\n",
      "Time required f/ MC rep  600 of 1000: 1.5015499591827393\n",
      "Time required f/ MC rep  700 of 1000: 1.2551307678222656\n",
      "Time required f/ MC rep  800 of 1000: 1.5193040370941162\n",
      "Time required f/ MC rep  900 of 1000: 1.338805913925171\n",
      "Time required f/ MC rep  1000 of 1000: 0.7257401943206787\n"
     ]
    }
   ],
   "source": [
    "N = 200\n",
    "sigmaU = 1\n",
    "sigmaA = 1/4\n",
    "eta = [-1, -1/2, 1/2]\n",
    "\n",
    "B = 1000\n",
    "\n",
    "theta    = np.zeros((B,3))\n",
    "coverage = np.zeros((B,9))\n",
    "se       = np.zeros((B,9))\n",
    "\n",
    "\n",
    "for b in range(0,B):\n",
    "    start = time.time()\n",
    "    \n",
    "    A = np.random.lognormal(-sigmaA/2, sigmaA, N)\n",
    "    X1 = np.random.uniform(0, 1, N)\n",
    "    X2 = np.random.uniform(0, 1, N)\n",
    "    X3 = np.random.uniform(0, 1, N)\n",
    "\n",
    "    D = []\n",
    "    W = []\n",
    "    for i, j in it.permutations(range(0,N), 2):\n",
    "        W_ij = ((X1[i] - X1[j])**2 + (X2[i] - X2[j])**2)**(1/2)\n",
    "        Y_ij = (np.exp(W_ij*eta[0] + X3[i]*eta[1] + X3[j]*eta[2]))*A[i]*A[j]*np.random.lognormal(-sigmaU/2, sigmaU)\n",
    "        W.append([Y_ij, W_ij, X3[i], X3[j], i, j])\n",
    "    \n",
    "\n",
    "    W = pd.DataFrame(W, columns=['Y_ij', 'W_ij', 'X3_i', 'X3_j', 'i', 'j'])    \n",
    "    W = W.set_index(['i', 'j'], drop = True)  # Set dataframe multi-index\n",
    "    W['constant'] = 1\n",
    "\n",
    "    # Get outcome variable and drop from regressor matrix\n",
    "    Y = W['Y_ij'].copy(deep=True)\n",
    "    W.drop('Y_ij', axis=1, inplace=True)\n",
    "    \n",
    "    # Standard errors based on independence across dyads\n",
    "    [theta_DR, vcov_theta_DR]= netrics.dyadic_regression(Y, W, regmodel='poisson', directed=True, nocons=True, \\\n",
    "                                                         silent=True, cov='ind')\n",
    "    \n",
    "    theta[b,:]    = theta_DR[0:3,:].T\n",
    "    \n",
    "    # Coverage\n",
    "    coverage[b,0] = (eta[0]<=theta_DR[0] + 1.96*np.sqrt(vcov_theta_DR[0,0]))*\\\n",
    "                    (eta[0]>=theta_DR[0] - 1.96*np.sqrt(vcov_theta_DR[0,0]))\n",
    "    \n",
    "    coverage[b,1] = (eta[1]<=theta_DR[1] + 1.96*np.sqrt(vcov_theta_DR[1,1]))*\\\n",
    "                    (eta[1]>=theta_DR[1] - 1.96*np.sqrt(vcov_theta_DR[1,1]))\n",
    "        \n",
    "    coverage[b,2] = (eta[2]<=theta_DR[2] + 1.96*np.sqrt(vcov_theta_DR[2,2]))*\\\n",
    "                    (eta[2]>=theta_DR[2] - 1.96*np.sqrt(vcov_theta_DR[2,2]))\n",
    "           \n",
    "    # Standard error length\n",
    "    se[b,0]       = np.sqrt(vcov_theta_DR[0,0])\n",
    "    se[b,1]       = np.sqrt(vcov_theta_DR[1,1])\n",
    "    se[b,2]       = np.sqrt(vcov_theta_DR[2,2])\n",
    "    \n",
    "    \n",
    "    # Jackknife standard errors\n",
    "    [theta_DR, vcov_theta_DR]= netrics.dyadic_regression(Y, W, regmodel='poisson', directed=True, nocons=True, \\\n",
    "                                                         silent=True, cov='DR')\n",
    "    \n",
    "    coverage[b,3] = (eta[0]<=theta_DR[0] + 1.96*np.sqrt(vcov_theta_DR[0,0]))*\\\n",
    "                    (eta[0]>=theta_DR[0] - 1.96*np.sqrt(vcov_theta_DR[0,0]))\n",
    "    \n",
    "    coverage[b,4] = (eta[1]<=theta_DR[1] + 1.96*np.sqrt(vcov_theta_DR[1,1]))*\\\n",
    "                    (eta[1]>=theta_DR[1] - 1.96*np.sqrt(vcov_theta_DR[1,1]))\n",
    "        \n",
    "    coverage[b,5] = (eta[2]<=theta_DR[2] + 1.96*np.sqrt(vcov_theta_DR[2,2]))*\\\n",
    "                    (eta[2]>=theta_DR[2] - 1.96*np.sqrt(vcov_theta_DR[2,2]))\n",
    "    \n",
    "    # Standard error length\n",
    "    se[b,3]       = np.sqrt(vcov_theta_DR[0,0])\n",
    "    se[b,4]       = np.sqrt(vcov_theta_DR[1,1])\n",
    "    se[b,5]       = np.sqrt(vcov_theta_DR[2,2])\n",
    "    \n",
    "    # Bias corrected standard errors\n",
    "    [theta_DR, vcov_theta_DR]= netrics.dyadic_regression(Y, W, regmodel='poisson', directed=True, nocons=True, \\\n",
    "                                                         silent=True, cov='DR_bc')\n",
    "    \n",
    "    coverage[b,6] = (eta[0]<=theta_DR[0] + 1.96*np.sqrt(vcov_theta_DR[0,0]))*\\\n",
    "                    (eta[0]>=theta_DR[0] - 1.96*np.sqrt(vcov_theta_DR[0,0]))\n",
    "    \n",
    "    coverage[b,7] = (eta[1]<=theta_DR[1] + 1.96*np.sqrt(vcov_theta_DR[1,1]))*\\\n",
    "                    (eta[1]>=theta_DR[1] - 1.96*np.sqrt(vcov_theta_DR[1,1]))\n",
    "        \n",
    "    coverage[b,8] = (eta[2]<=theta_DR[2] + 1.96*np.sqrt(vcov_theta_DR[2,2]))*\\\n",
    "                    (eta[2]>=theta_DR[2] - 1.96*np.sqrt(vcov_theta_DR[2,2]))\n",
    "    \n",
    "    # Standard error length\n",
    "    se[b,6]       = np.sqrt(vcov_theta_DR[0,0])\n",
    "    se[b,7]       = np.sqrt(vcov_theta_DR[1,1])\n",
    "    se[b,8]       = np.sqrt(vcov_theta_DR[2,2])\n",
    "        \n",
    "    end = time.time()\n",
    "    if (b+1) % 100 == 0:\n",
    "        print(\"Time required f/ MC rep  \" + str(b+1) + \" of \" + str(B) + \": \" + str(end-start))      \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following table reports Monte Carlo coverage estimates for Wald type nominal 95 percent confidence intervals based on the three variance estimates available in **dyadic_regression()**. The first assumes independence across dyads ('ind'), the second is the jackknife estimate ('DR'), and the third a bias corrected estimate ('DR_bc'). These last two allow for dependence across dyads sharing at least one agent in common. The columns in the table correspond to the three parameters in the model, the rows to the three variance estimators.   \n",
    "\n",
    "The table shows that assuming independence across dyads results in considerable undercoverage. The jackknife estimate, as would be expected, is conservative. Finally the biased corrected estimate has actual coverage very close to nominal coverage.   \n",
    "\n",
    "The standard error associated with a Monte Carlo coverage estimate is $\\sqrt{\\alpha\\left(1-\\alpha\\right)/B}$. With $B = 1,000$ simulations and $\\alpha = 0.05$ this results in a standard error of approximately 0.007 or a 95 percent confidence interval of $[0.936, 0.964]$. Hence the 'ind' standard errors significantly undercover, while the 'DR' standard errors (sometimes) significantly overcover. The 'DR_bc' have actual coverage (almost) insignificantly different from target coverage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.75  0.551 0.529]\n",
      " [0.976 0.956 0.953]\n",
      " [0.936 0.94  0.943]]\n"
     ]
    }
   ],
   "source": [
    "print(np.mean(coverage,axis=0).reshape((3,3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we look at average standard error length. The 'DR_bc' standard errors, while appreciably large than those base bone the (incorrect) assumption of independence across dyads, are smaller than the jackknife estimates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.03178332 0.02629973 0.0263127 ]\n",
      " [0.05967391 0.07120008 0.07101688]\n",
      " [0.05042528 0.06613525 0.06593012]]\n"
     ]
    }
   ],
   "source": [
    "print(np.mean(se,axis=0).reshape((3,3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "\n",
       "html {\n",
       "  font-size: 62.5% !important; }\n",
       "body {\n",
       "  font-size: 1.5em !important; /* currently ems cause chrome bug misinterpreting rems on body element */\n",
       "  line-height: 1.6 !important;\n",
       "  font-weight: 400 !important;\n",
       "  font-family: \"Raleway\", \"HelveticaNeue\", \"Helvetica Neue\", Helvetica, Arial, sans-serif !important;\n",
       "  color: #222 !important; }\n",
       "\n",
       "div{ border-radius: 0px !important;  }\n",
       "div.CodeMirror-sizer{ background: rgb(244, 244, 248) !important; }\n",
       "div.input_area{ background: rgb(244, 244, 248) !important; }\n",
       "\n",
       "div.out_prompt_overlay:hover{ background: rgb(244, 244, 248) !important; }\n",
       "div.input_prompt:hover{ background: rgb(244, 244, 248) !important; }\n",
       "\n",
       "h1, h2, h3, h4, h5, h6 {\n",
       "  color: #333 !important;\n",
       "  margin-top: 0 !important;\n",
       "  margin-bottom: 2rem !important;\n",
       "  font-weight: 300 !important; }\n",
       "h1 { font-size: 4.0rem !important; line-height: 1.2 !important;  letter-spacing: -.1rem !important;}\n",
       "h2 { font-size: 3.6rem !important; line-height: 1.25 !important; letter-spacing: -.1rem !important; }\n",
       "h3 { font-size: 3.0rem !important; line-height: 1.3 !important;  letter-spacing: -.1rem !important; }\n",
       "h4 { font-size: 2.4rem !important; line-height: 1.35 !important; letter-spacing: -.08rem !important; }\n",
       "h5 { font-size: 1.8rem !important; line-height: 1.5 !important;  letter-spacing: -.05rem !important; }\n",
       "h6 { font-size: 1.5rem !important; line-height: 1.6 !important;  letter-spacing: 0 !important; }\n",
       "\n",
       "@media (min-width: 550px) {\n",
       "  h1 { font-size: 5.0rem !important; }\n",
       "  h2 { font-size: 4.2rem !important; }\n",
       "  h3 { font-size: 3.6rem !important; }\n",
       "  h4 { font-size: 3.0rem !important; }\n",
       "  h5 { font-size: 2.4rem !important; }\n",
       "  h6 { font-size: 1.5rem !important; }\n",
       "}\n",
       "\n",
       "p {\n",
       "  margin-top: 0 !important; }\n",
       "  \n",
       "a {\n",
       "  color: #1EAEDB !important; }\n",
       "a:hover {\n",
       "  color: #0FA0CE !important; }\n",
       "  \n",
       "code {\n",
       "  padding: .2rem .5rem !important;\n",
       "  margin: 0 .2rem !important;\n",
       "  font-size: 90% !important;\n",
       "  white-space: nowrap !important;\n",
       "  background: #F1F1F1 !important;\n",
       "  border: 1px solid #E1E1E1 !important;\n",
       "  border-radius: 4px !important; }\n",
       "pre > code {\n",
       "  display: block !important;\n",
       "  padding: 1rem 1.5rem !important;\n",
       "  white-space: pre !important; }\n",
       "  \n",
       "button{ border-radius: 0px !important; }\n",
       ".navbar-inner{ background-image: none !important;  }\n",
       "select, textarea{ border-radius: 0px !important; }\n",
       "\n",
       "</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This imports an attractive notebook style from Github\n",
    "from IPython.display import HTML\n",
    "from urllib.request import urlopen\n",
    "html = urlopen('http://bit.ly/1Bf5Hft')\n",
    "HTML(html.read().decode('utf-8'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
